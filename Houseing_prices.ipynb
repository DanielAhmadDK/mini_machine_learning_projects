{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data set contains 14 features including the target, which is the sales price, of houses in the boston area in a time period from 1950s up to present time. Features such as garage, quality, how many rooms and many other were documented each time a house was built and sold. The objective here is to develop a model that can predict the selling price of a house. For this, a test, training and submission set are provided to train the model, test it and finally using the submission set to see how well the accuracy is. The steps will start of by creating the dataframes, performing an EDA by constructing graphs to visualize any trends, data cleaning and preparation, and finally model constructing."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## The first part is to create two dataframes 1) for training data 2) for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nsub_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Import packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style='dark')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\n\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n#feature selection modules\nfrom sklearn.feature_selection import SelectKBest # Univariate Feature Selection\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import chi2 # To apply Univariate Feature Selection\nfrom sklearn.feature_selection import RFE # Recursive Feature Selection\nfrom sklearn.decomposition import PCA # To apply PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### The business goal is to develop a model that can predict housing prices. So the target is price of the house.\n"},{"metadata":{},"cell_type":"markdown","source":"# 1) Understanding the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def detailed_analysis(df, pred=None):\n  obs = df.shape[0]\n  types = df.dtypes\n  counts = df.apply(lambda x: x.count())\n  uniques = df.apply(lambda x: [x.unique()])\n  nulls = df.apply(lambda x: x.isnull().sum())\n  distincts = df.apply(lambda x: x.unique().shape[0])\n  missing_ratio = (df.isnull().sum() / obs) * 100\n  skewness = df.skew()\n  kurtosis = df.kurt()\n  print('Data shape:', df.shape)\n\n  if pred is None:\n    cols = ['types', 'counts', 'nulls', 'distincts', 'missing ratio', 'uniques', 'skewness', 'kurtosis']\n    details = pd.concat([types, counts, nulls, distincts, missing_ratio, uniques, skewness, kurtosis], axis=1)\n  else:\n    corr = df.corr()[pred]\n    details = pd.concat([types, counts, nulls, distincts, missing_ratio, uniques, skewness, kurtosis, corr], axis=1\n                        , sort=False)\n    corr_col = 'corr ' + pred\n    cols = ['types', 'counts', 'nulls', 'distincts', 'missing ratio', 'uniques', 'skewness', 'kurtosis', corr_col]\n\n  details.columns = cols\n  dtypes = details.types.value_counts()\n  print('____________________________\\nData types:\\n', dtypes)\n  print('____________________________')\n  return details\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"details = detailed_analysis(train_df, 'SalePrice')\ndisplay(details.sort_values(by='corr SalePrice', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Highly positive skewed (skewed to the right)----------\\n:', details.query('skewness > 1.4'))\nprint('Skewed to the left-----------\\n:', details.query('skewness < -1')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_col = [col for col in train_df.columns if train_df[col].dtypes != 'O']\ncat_col = [col for col in train_df.columns if train_df[col].dtypes == 'O']\nprint(num_col)\nprint(cat_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There are several observations we can make from the deatailed table above\n* Starting with the dependent variable, we can see that it has a high positive skewness (skewed to the right) value which would mean we need to consider a transformation. Some possible transformations include: square root, cube root and log. There is always a debate about whether it is necessary to obtain a normal distribution. I have read mixed opinions on this topic where some say it is not necessary, and others say of course it is dependent on the type of model being used. In this case, since we are trying to predict the price, then the type of models would be a regression, most likely a linear regression as the title says. There are other variables that are skewed to the left or right, but some maybe could be removed because there is no significance.\n* Next, we look at the type of variables. There are both numerical and categorical features, so they need to be separately analyzed\n\n* There are missing values for each feature type, so this also needs to be dealt with before we start constructing our models.\n\n* Feature selection will also be necessary since there are way too many features, and some are not significant\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Numerical features analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ntarget = 'SalePrice'\nnc = 4\nnr = math.ceil(len(num_col)/nc)\n\ni = 0\nj = 0\n\nfigure, ax = plt.subplots(nrows = nr, ncols = nc, figsize = (20,5*nr))\nfor value in num_col:\n    ax[i,j].set_xlabel(value)\n    ax[i,j].set_ylabel(target)\n    ax[i,j].set_title(value + 'vs' + target)\n    ax[i,j].scatter(x = train_df[[value]], y = train_df[[target]], alpha = 0.3)\n    if j == nc-1:\n        i = i + 1\n        j = 0\n    else:\n        j = j + 1\n    figure.tight_layout(pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we can get out of the plots is a visual understanding of the correlation between our target and numerical features. However, some of the features are discrete values, some also seem to have high concentration of values in one area. This could be a sign of possible transformation of the features, or maybe even some anomalies/outliers. However, one thing is for sure, it can already be seen here that certain numerical features can be dropped."},{"metadata":{},"cell_type":"markdown","source":"## Categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x = plt.xticks(rotation=90)\n\ng = sns.FacetGrid(\n    pd.melt(train_df.fillna(\"MISSING\"), id_vars=['SalePrice'], value_vars = cat_col),\n    col='variable',\n    col_wrap=4,\n    sharex=False,\n    sharey=False,\n    height=5\n).map(boxplot, 'value', 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* MSZoning plot: There are clearly a few outliears, and it also appears that there is no strong influence between the MSZoning and the saleprice of a house\n* Street plot: Again, clearly outliers are present, but only for pave, and there seems to be no strong influence on the house price.\n* Alley plot: Same conclusion, outliers and no strong influence\n\n* Without going into to much detail with each plot, it appears that several features does not have a strong influence on the house of the prices. The utlities feature can possibly be removed since there is only one observation. Neighboorhood does have a strong influence on the house price, which also makes sense. If it is a neighborhood with a high crime rate, then this would have a negative influence on the price. Or if the neighboorhood is perfect for a family, young or old couples this would also influence the price. Condition of the house also seems to have an influence on the price, which is realistic. Roof material has a influence. This is a big factor in purchasing a home since changing the roof is an expensive affair, so it seems reasonable that the type of material used will affect the price. Exterior1st appears to have some influence on the price, but the majority is around the 200k-300k. Exterqual appears to have an influence. Basment quality, and pool quality appear to have an influence on the price of the house. Finally, the saletype also seems to have an influence. Conclusion is, there also can be seen signs that certain features could be removed."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Filling in missing values"},{"metadata":{},"cell_type":"markdown","source":"I will first look at the categorical features. There are many features that using ratings in the form as strings to \nevaulate the quality of a given attribute. I will use 1-5 to convert the ratings where 1 is the worst, and 5 is the best. This will be done using a loop for both test and training sets to make it easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# to categorical feature\ncols = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\n        \"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\n        \"LowQualFinSF\",\"GarageYrBlt\"]\n\nfor c in cols:\n    train_df[c] = train_df[c].astype(str)\n    test_df[c] = test_df[c].astype(str)\n    \n\n# encode quality\n# Ex(Excellent), Gd（Good）, TA（Typical/Average）, Fa（Fair）, Po（Poor）\ncols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC',\n        'KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']\nfor c in cols:\n    train_df[c].fillna(0, inplace=True)\n    test_df[c].fillna(0, inplace =True)\n    \n    train_df[c].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    test_df[c].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pair_features_to_dummies(df, col1, col2, prefix):\n    d_1 = pd.get_dummies(df[col1].astype(str), prefix=prefix)\n    d_2 = pd.get_dummies(df[col2].astype(str), prefix=prefix)\n    for c in list(set(list(d_1.columns) + list(d_2.columns))):\n        if not c in d_1.columns: d_1[c] = 0\n        if not c in d_2.columns: d_2[c] = 0\n    return (d_1 + d_2).clip(0, 1)\n\ncond = pair_features_to_dummies(train_df,'Condition1','Condition2','Condition')\ncond = pair_features_to_dummies(test_df,'Condition1','Condition2','Condition')\nexterior = pair_features_to_dummies(train_df,'Exterior1st','Exterior2nd','Exterior')\nexterior = pair_features_to_dummies(test_df,'Exterior1st','Exterior2nd','Exterior')\nbsmtftype = pair_features_to_dummies(train_df,'BsmtFinType1','BsmtFinType2','BsmtFinType') \nbsmtftype = pair_features_to_dummies(test_df,'BsmtFinType1','BsmtFinType2','BsmtFinType') \n\ntrain_new = pd.concat([train_df, cond, exterior, bsmtftype], axis=1)\ntest_new = pd.concat([train_df, cond, exterior, bsmtftype], axis=1)\ntrain_new.drop(['Condition1','Condition2', 'Exterior1st','Exterior2nd',\n               'BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)\ntest_new.drop(['Condition1','Condition2', 'Exterior1st','Exterior2nd',\n               'BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)\ntrain_new.head(10)\ntest_new.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_null = train_new.isnull().sum()\npercent = (train_new.isnull().sum()/train_new.isnull().count())\ntotal_not_null = train_new.notnull().sum()\n\nmissing_data = pd.concat([total_null, percent, total_not_null], axis = 1, keys = [\"Total_null\", \"Percent of missing values\", \"total_not_null\"]) #axis = 1 create two columns, one for total and one for percent\n\nprint(missing_data.loc[missing_data[\"Total_null\"] > 0].sort_values(by = 'Percent of missing values', ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking at how many values are missing, I think it is justifiable to drop the top three features. Of course,\nit is never optimal to drop data. But in this case where over 88% is missing, then filling the values would maybe create some false conclusions later on. What we could do is fill in the values for now, and issue a feature selection later. Lets try that first."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['MiscFeature', 'Alley', 'Fence']:\n    train_new[c].fillna('None', inplace=True)\n    test_new[c].fillna('None', inplace=True)\n    \ntrain_new['LotFrontage'] = train_new.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest_new['LotFrontage'] = test_new.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ntrain_new.loc[train_new.GarageYrBlt.isnull(),'GarageYrBlt'] = train_new.loc[train_new.GarageYrBlt.isnull(),'YearBuilt']\ntest_new.loc[test_new.GarageYrBlt.isnull(),'GarageYrBlt'] = test_new.loc[test_new.GarageYrBlt.isnull(),'YearBuilt']\n\n\ntrain_new['GarageType'].fillna('None', inplace=True)\ntrain_new['GarageFinish'].fillna(0, inplace=True)\ntest_new['GarageType'].fillna('None', inplace=True)\ntest_new['GarageFinish'].fillna(0, inplace=True)\n\nfor c in ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    train_new[c].fillna(0, inplace=True)\n    test_new[c].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\nfor i, t in train_new.loc[:, train_new.columns != 'SalePrice'].dtypes.iteritems():\n    if t == object:\n        train_new[i].fillna(train_new[i].mode()[0], inplace=True)\n        train_new[i] = LabelEncoder().fit_transform(train_new[i].astype(str))\n    else:\n        train_new[i].fillna(train_new[i].median(), inplace=True)\n        \nfor i, t in test_new.loc[:, test_new.columns != 'SalePrice'].dtypes.iteritems():\n    if t == object:\n        test_new[i].fillna(train_new[i].mode()[0], inplace=True)\n        test_new[i] = LabelEncoder().fit_transform(test_new[i].astype(str))\n    else:\n        test_new[i].fillna(test_new[i].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new.head(5)\ntest_new.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now, all strings are converted, and missing values are delt with. Lets try and do some feature selection to reduce the number of features. First, we split the training set into train and test. We should be cautious that this is still the training set. We also have a test test set that we will use to test our model before using the submission data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_new.drop(['SalePrice'], axis = 1)\ny = train_new['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandForest_RFE = RandomForestClassifier()\nrfe = RFE(estimator = RandForest_RFE, n_features_to_select = 5, step = 1)\nrfe = rfe.fit(X_train, y_train)\n\nprint('Best features chosen by RFE: \\n')\nfor i in X_train.columns[rfe.support_]:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That is one way to do it. Another way is the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"RandForest_RFECV = RandomForestClassifier()\nrfecv = RFECV(estimator = RandForest_RFECV, step = 1, cv = 3, scoring = 'accuracy')\nrfecv = rfecv.fit(X_train, y_train)\nprint('Best number of features:', rfecv.n_features_)\nprint('Features :\\n')\nfor i in X_train.columns[rfecv.support_]:\n    print(i)\n    \nplt.figure()\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score of Selected Features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=80)\nfit = bestfeatures.fit(X_train,y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_train.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(80,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop = ['PoolQC','MiscFeature','Alley','Fence','Utilities']\n#for i in drop:\n#    X_train = X_train.drop(columns = i)\n#    X_test = X_test.drop(columns = i)\n#    test_df = test_df.drop(columns = i)\n#    print(i + \"removed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_null = train_df.isnull().sum()\npercent = (train_df.isnull().sum()/train_df.isnull().count())\ntotal_not_null = train_df.notnull().sum()\n\nmissing_data = pd.concat([total_null, percent, total_not_null], axis = 1, keys = [\"Total_null\", \"Percent of missing values\", \"total_not_null\"]) #axis = 1 create two columns, one for total and one for percent\n\nprint(missing_data.loc[missing_data[\"Total_null\"] > 0].sort_values(by = 'Percent of missing values', ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that PoolQC, MiscFeature, Alley, and Fence have extremely high percent of missing values. This could be that the house does not have this feature, or that the value was not recorded. However, it could be justified to exclude these features from our model, since filling the missing values would give a bias result."},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop = ['PoolQC','MiscFeature','Alley','Fence','Utilities']\n#for i in drop:\n#    X_train = X_train.drop(columns = i)\n#    X_test = X_test.drop(columns = i)\n#    test_df = test_df.drop(columns = i)\n#    print(i + \"removed\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = [col for col in X_train.columns if X_train[col].dtypes!= 'O']\ncategorical_features =[col for col in X_train.columns if X_train[col].dtypes== 'O']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder,LabelBinarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#most_common = X_train['Electrical'].mode()\n#X_train['Electrical'] = X_train['Electrical'].fillna(most_common)\n\n#X_train[\"MasVnrArea\"].fillna(0, inplace = True)\n#X_train[\"MasVnrType\"].fillna(\"None\", inplace = True)\n\n#X_test[\"MasVnrArea\"].fillna(0, inplace = True)\n#X_test[\"MasVnrType\"].fillna(\"None\", inplace = True)\n\n#test_df[\"MasVnrArea\"].fillna(0, inplace = True)\n#test_df[\"MasVnrType\"].fillna(\"None\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#X_train[\"FireplaceQu\"].fillna(\"None\", inplace = True)\n#X_test[\"FireplaceQu\"].fillna(\"None\", inplace = True)\n#test_df[\"FireplaceQu\"].fillna(\"None\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class new_LabelBina(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.encoder = OneHotEncoder()\n    def fit(self, X, y=None):\n        self.encoder.fit(X)\n        return self\n    def transform(self, X):\n        return self.encoder.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_pipeline = Pipeline(steps = [('imputer', SimpleImputer(strategy = 'median')),\n                                     ('scaler', StandardScaler())])\n\ntarget_pipeline = Pipeline(steps = [('target_scale', StandardScaler())])\n\ncategorical_pipeline = Pipeline(steps = [('label_encoder', new_LabelBina()),\n                                        ('imputer_cat', SimpleImputer(missing_data = 'nan', \n                                                                      strategy = 'constant', fill_value = 0))])\n\nfull_pipeline = ColumnTransformer(transformers=[('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features),\n                                              ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start with feature selection to see if there are any features that can be disregarded"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_prep=full_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to scale the output"},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_output=StandardScaler()\nnew_target=scale_output.fit_transform(np.array(y_train).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandForest_RFECV = RandomForestClassifier()\nrfecv = RFECV(estimator = RandForest_RFECV, step = 1, cv = 3, scoring = 'accuracy')\nrfecv = rfecv.fit(house_data_prep, new_target)\nprint('Best number of features:', rfecv.n_features_)\nprint('Features :\\n')\nfor i in X_train.columns[rfecv.support_]:\n    print(i)\n    \nplt.figure()\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score of Selected Features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now, the full pipeline is constructed and fitted on the training set, so we now can use house_data_prepared to train our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"linear=LinearRegression()\nscale_output=StandardScaler()\nnew_y=scale_output.fit_transform(np.array(y_train).reshape(-1,1))\nlinear.fit(spotify_preparared,new_y)\ntest_data=full_pipeline.transform(X_test)\npredictions=linear.predict(test_data)\nfrom sklearn.metrics import mean_squared_error\nnew_y=scale_output.transform(np.array(y_test).reshape(-1,1))\n\nlin_mse = np.sqrt(mean_squared_error(new_y,predictions))\nprint(lin_mse)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}